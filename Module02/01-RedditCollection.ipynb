{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Collecting Reddit Data with PRAW\n",
    "\n",
    "This notebook contains examples for using web-based APIs (Application Programmer Interfaces) to download data from social media platforms.\n",
    "\n",
    "This notebook focuses specifically on _Reddit_.\n",
    "\n",
    "For most services, we need to register with the platform in order to use their API.\n",
    "Instructions for the registration processes are outlined in each specific section below.\n",
    "\n",
    "We will use APIs because they *can* be much faster than manually copying and pasting data from the web site, APIs provide uniform methods for accessing resources (searching for keywords, places, or dates), and it should conform to the platform's terms of service (important for partnering and publications).\n",
    "Note however that each of these platforms has strict limits on access times: e.g., requests per hour, search history depth, maximum number of items returned per request, and similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Reddit API\n",
    "\n",
    "Reddit's API used to be the easiest to use since it did not require credentials to access data on its subreddit pages.\n",
    "Unfortunately, this process has been changed, and developers now need to create a Reddit application on Reddit's app page located here: (https://www.reddit.com/prefs/apps/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our first piece of code, we need to import the package \n",
    "# that connects to Reddit. Praw is a thin wrapper around reddit's \n",
    "# web APIs and works well\n",
    "\n",
    "import praw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Reddit Application\n",
    "Go to https://www.reddit.com/prefs/apps/.\n",
    "Scroll down to \"create application\", select \"web app\", and provide a name, description, and URL (which can be anything).\n",
    "\n",
    "After you press \"create app\", you will be redirected to a new page with information about your application. Copy the unique identifiers below \"web app\" and beside \"secret\". These are your client_id and client_secret values, which you need below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we specify a \"unique\" user agent for our code\n",
    "# This is primarily for identification, I think, and some\n",
    "# user-agents of bad actors might be blocked\n",
    "redditApi = praw.Reddit(client_id='xxx',\n",
    "                        client_secret='xxx',\n",
    "                        user_agent='is688_cbuntain_v01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capturing Reddit Posts\n",
    "\n",
    "Now for a given subreddit, we can get the newest posts to that sub. \n",
    "Post titles are generally short, so you could treat them as something similar to a tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = \"worldnews\"\n",
    "\n",
    "targetSub = redditApi.subreddit(subreddit)\n",
    "\n",
    "submissions = targetSub.new(limit=10)\n",
    "for post in submissions:\n",
    "    print(post.title, post.author.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = \"equality\"\n",
    "targetSub = redditApi.subreddit(subreddit)\n",
    "\n",
    "# Array for storing subreddit post titles\n",
    "title_list = []\n",
    "submissions = targetSub.new(limit=10)\n",
    "for post in submissions:\n",
    "        title_list.append(post.title)\n",
    "\n",
    "print(title_list)\n",
    "\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "for post_title in title_list:\n",
    "    tokenizer.tokenize(post_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leveraging Reddit's Voting\n",
    "\n",
    "Getting the new posts gives us the most up-to-date information. \n",
    "You can also get the \"hot\" posts, \"top\" posts, etc. that should be of higher quality. \n",
    "In theory.\n",
    "__Caveat emptor__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = \"worldnews\"\n",
    "\n",
    "targetSub = redditApi.subreddit(subreddit)\n",
    "\n",
    "submissions = targetSub.hot(limit=5)\n",
    "for post in submissions:\n",
    "    print(post.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following Multiple Subreddits\n",
    "\n",
    "Reddit has a mechanism called \"multireddits\" that essentially allow you to view multiple reddits together as though they were one.\n",
    "To do this, you need to concatenate your subreddits of interesting using the \"+\" sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = \"worldnews+news\"\n",
    "\n",
    "targetSub = redditApi.subreddit(subreddit)\n",
    "submissions = targetSub.new(limit=10)\n",
    "for post in submissions:\n",
    "    print(post.title, post.author)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Reddit Comments\n",
    "\n",
    "While you're never supposed to read the comments, for certain live streams or new and rising posts, the comments may provide useful insight into events on the ground or people's sentiment.\n",
    "New posts may not have comments yet though.\n",
    "\n",
    "Comments are attached to the post title, so for a given submission, you can pull its comments directly.\n",
    "\n",
    "Note Reddit returns pages of comments to prevent server overload, so you will not get all comments at once and will have to write code for getting more comments than the top ones returned at first.\n",
    "This pagination is performed using the MoreXYZ objects (e.g., MoreComments or MorePosts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "subreddit = \"worldnews\"\n",
    "\n",
    "breadthCommentCount = 5\n",
    "\n",
    "targetSub = redditApi.subreddit(subreddit)\n",
    "\n",
    "submissions = targetSub.hot(limit=1)\n",
    "\n",
    "for post in submissions:\n",
    "    print (post.title)\n",
    "    \n",
    "    post.comment_limit = breadthCommentCount\n",
    "    \n",
    "    # Get the top few comments\n",
    "    for comment in post.comments.list():\n",
    "        if isinstance(comment, praw.models.MoreComments):\n",
    "            continue\n",
    "        \n",
    "        print (\"---\", comment.name, \"---\")\n",
    "        print (\"\\t\", comment.body)\n",
    "        \n",
    "        for reply in comment.replies.list():\n",
    "            if isinstance(reply, praw.models.MoreComments):\n",
    "                continue\n",
    "            \n",
    "            print (\"\\t\", \"---\", reply.name, \"---\")\n",
    "            print (\"\\t\\t\", reply.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Functionality\n",
    "\n",
    "Reddit has a deep comment structure, and the code above only goes two levels down (top comment and top comment reply). \n",
    "You can view Praw's additional functionality, replete with examples on its website here: http://praw.readthedocs.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
